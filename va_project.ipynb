{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/thinkdeep/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/thinkdeep/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from tensorflow.keras.models import Sequential, Model, Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Input, add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the Flickr8k images and caption files\n",
    "dataset_image_path =\"flickr8k/Images/\"\n",
    "dataset_text_path  =\"flickr8k/captions.txt\" \n",
    "# Wanted shape for images\n",
    "wanted_shape = (224,224,3)\n",
    "\n",
    "# TO WORK ON A REDUCED DATASET\n",
    "#n_images_considered = 899 # = 8091 / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the text dataset corresponding to images\n",
    "train, infer = True, False\n",
    "\n",
    "if train:\n",
    "    df_texts = pd.read_csv(dataset_text_path, sep=\",\") #[\"image\",\"caption\"] \n",
    "elif infer:\n",
    "    df_texts = pd.read_csv(\"text_feature_maps.csv\") # [\"image\",\"caption\",\"cleaned\",\"cleaned_tokenized\",\"embedded\"]\n",
    "\n",
    "n_img = df_texts.count()/5 # 40455/5 \n",
    "unique_img = pd.unique(df_texts[\"image\"])# 8091 unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing images with pretrained VGG16 : FEATURE MAPS 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape=wanted_shape, pooling=None, classes=1000\n",
    ")\n",
    "# Feature extraction\n",
    "vgg_model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output) #end the mod√®le with a 4096 feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Image feature maps loaded : (8091, 4096)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "charge_image, one_by_one, load_fm_image_csv = False, False, True# false to gain time when testing other parts\n",
    "# To obtain the feature maps\n",
    "if charge_image:\n",
    "    feature_maps = np.array([vgg_model.predict(load_img_from_ds(unique_img[i])) for i in range(len(unique_img))])\n",
    "    print(f\"Shape des fm {feature_maps.shape}\")\n",
    "\n",
    "elif one_by_one:\n",
    "    feature_maps=[]\n",
    "    for i in range(len(unique_img)):\n",
    "        if i!=0:\n",
    "            print(f\"{i}/{len(unique_img)} - time elapsed :{time.time()-a}\")\n",
    "        else:\n",
    "            print(f\"{i}/{len(unique_img)}\")\n",
    "        a=time.time()\n",
    "        img = load_img_from_ds(unique_img[i])\n",
    "        feature_map = vgg_model.predict(img)\n",
    "        feature_maps.append(feature_map)\n",
    "    feature_maps=np.array(feature_maps)\n",
    "    #save to csv\n",
    "    feature_maps_sav=feature_maps[:,0,:]\n",
    "    df_fm = pd.DataFrame(feature_maps_sav)\n",
    "    df_fm.to_csv(\"image_feature_maps.csv\")\n",
    "\n",
    "elif load_fm_image_csv:\n",
    "    df_fm = pd.read_csv(\"image_feature_maps.csv\")\n",
    "    feature_maps = np.array(df_fm.drop([df_fm.columns[0]], axis=1))\n",
    "    print(f\"Image feature maps loaded : {feature_maps.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing captions - WORD2VEC : EMBEDDINGS 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not enough memory yet\n",
    "word2vec_training = False\n",
    "\n",
    "if train :\n",
    "    # Text preprocessing\n",
    "    df_texts[\"cleaned\"]=[process_sentence(s) for s in df_texts[\"caption\"]]\n",
    "    df_texts[\"cleaned_tokenized\"]=[word_tokenize(w) for w in df_texts[\"cleaned\"]]\n",
    "    if(word2vec_training):\n",
    "        import logging\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        word2vec_model = gensim.models.Word2Vec([word_tokenize(w) for w in df_texts[\"cleaned\"]], min_count=1, size=4096)\n",
    "        word2vec_model.save(\"word2vec.model\")\n",
    "    else:\n",
    "        word2vec_model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
    "    df_texts[\"embedded\"] = word2vec(df_texts,word2vec_model.wv)\n",
    "    df_texts.to_csv(\"text_feature_maps.csv\", columns=df_texts.columns)\n",
    "\n",
    "elif infer:\n",
    "    word2vec_model = gensim.models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-13 17:35:18,199 : INFO : loading Word2Vec object from word2vec.model\n",
      "2021-01-13 17:35:18,257 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2021-01-13 17:35:18,278 : INFO : loading vectors from word2vec.model.wv.vectors.npy with mmap=None\n",
      "2021-01-13 17:35:18,519 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-01-13 17:35:18,525 : INFO : loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "2021-01-13 17:35:18,531 : INFO : loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "2021-01-13 17:35:18,536 : INFO : loading syn1neg from word2vec.model.trainables.syn1neg.npy with mmap=None\n",
      "2021-01-13 17:35:18,748 : INFO : setting ignored attribute cum_table to None\n",
      "2021-01-13 17:35:18,768 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gensim\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_model = gensim.models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts[\"cleaned\"]=[process_sentence(s) for s in df_texts[\"caption\"]]\n",
    "df_texts[\"cleaned_tokenized\"]=[word_tokenize(w) for w in df_texts[\"cleaned\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \\\n",
       "0  A child in a pink dress is climbing up a set o...   \n",
       "1              A girl going into a wooden building .   \n",
       "2   A little girl climbing into a wooden playhouse .   \n",
       "3  A little girl climbing the stairs to her playh...   \n",
       "4  A little girl in a pink dress going into a woo...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  startseq child pink dress climbing set stairs ...   \n",
       "1        startseq girl going wooden building  endseq   \n",
       "2  startseq little girl climbing wooden playhouse...   \n",
       "3  startseq little girl climbing stairs playhouse...   \n",
       "4  startseq little girl pink dress going wooden c...   \n",
       "\n",
       "                                   cleaned_tokenized  \n",
       "0  [startseq, child, pink, dress, climbing, set, ...  \n",
       "1  [startseq, girl, going, wooden, building, endseq]  \n",
       "2  [startseq, little, girl, climbing, wooden, pla...  \n",
       "3  [startseq, little, girl, climbing, stairs, pla...  \n",
       "4  [startseq, little, girl, pink, dress, going, w...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n      <th>cleaned</th>\n      <th>cleaned_tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n      <td>startseq child pink dress climbing set stairs ...</td>\n      <td>[startseq, child, pink, dress, climbing, set, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n      <td>startseq girl going wooden building  endseq</td>\n      <td>[startseq, girl, going, wooden, building, endseq]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n      <td>startseq little girl climbing wooden playhouse...</td>\n      <td>[startseq, little, girl, climbing, wooden, pla...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n      <td>startseq little girl climbing stairs playhouse...</td>\n      <td>[startseq, little, girl, climbing, stairs, pla...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n      <td>startseq little girl pink dress going wooden c...</td>\n      <td>[startseq, little, girl, pink, dress, going, w...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec(df_texts,word2vec_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'feature_maps' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3546305cfc3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dimages = np.array([np.array(load_img_from_ds(unique_img[i])) for i in range(len(unique_img))])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#dimages = np.array([np.array(load_img_from_ds(df_texts[\"image\"][i])) for i in range(len(df_texts[\"image\"]))])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Split du dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_maps' is not defined"
     ]
    }
   ],
   "source": [
    "dimages = feature_maps\n",
    "\n",
    "# Split du dataset\n",
    "prop_test, prop_val = 0.2, 0.2\n",
    "N = len(df_texts[\"embedded\"])\n",
    "Ntest, Nval = int(N*prop_test), int(N*prop_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = true image caption cleaned\n",
    "dt_test, dt_val, dt_train = split_test_val_train(df_texts[\"embedded\"], Ntest, Nval)\n",
    "# di = true image array\n",
    "di_test, di_val, di_train = split_test_val_train(dimages, Ntest, Nval)\n",
    "# fnm = image_name\n",
    "fnm_test, fnm_val, fnm_train = split_test_val_train(df_texts[\"image\"], Ntest, Nval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ab055cb38a09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mXtext_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXimage_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytext_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinalpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdi_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mXtext_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXimage_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytext_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinalpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdi_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\3√®me Ann√©e\\Semestre_9\\Vision_artificielle\\projet_vision_artificielle\\utils.py\u001b[0m in \u001b[0;36mfinalpreprocessing\u001b[1;34m(dftext, dfimage, vocab_size)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinalpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"# captions/images = {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# return error if len(text) != len(image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dtext' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2vec_model.wv.vocab)\n",
    "\n",
    "Xtext_train, Ximage_train, ytext_train = finalpreprocessing(dt_train, di_train, vocab_size) \n",
    "Xtext_val, Ximage_val, ytext_val = finalpreprocessing(dt_val, di_val, vocab_size)\n",
    "\n",
    "print(f\"Training set : \\n \\tInput image : {Ximage_train.shape}\\n\\tInput text : {Xtext_train.shape}\\n\\tOutput text : {ytext_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedding=64\n",
    "\n",
    "# image input\n",
    "input_img = Input(shape=(Ximage_train.shape[1],), name=\"InputImage\") )\n",
    "input_img = ( Dense(units=256,activation='relu',name=\"CompressedImageFeatures\") )(input_img)\n",
    "# text input\n",
    "input_txt = Input(shape=(maxlen,), name=\"InputSequence\"))\n",
    "input_txt = ( Embedding(vocab_size,dim_embedding, mask_zero=True))(input_txt)\n",
    "input_txt = ( LSTM(units=8, activation=\"relu\", name=\"CaptionFeatures\") )(input_txt)\n",
    "\n",
    "# Common part\n",
    "common = add(input_txt, input_img)\n",
    "common = Dense(256, activation='relu') (common)\n",
    "common = Dense(vocab_size, activation='softmax')(common)\n",
    "\n",
    "#Model\n",
    "total_model  = Model(inputs=[input_image, input_txt],outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit([Ximage_train, Xtext_train], ytext_train, epochs=5, verbose=2, batch_size=64, validation_data=([Ximage_val, Xtext_val], ytext_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\", \"val_loss\"]:\n",
    "    plt.plot(hist.history[label], label=label)]\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PREDICTION\n",
    "'''\n",
    "\n",
    "# 1 couche 256 LSTM ?\n",
    "# A partir de combien de couce=hes c est ok 1 8 16 32 256 \n",
    "# Temps d entrainement : compromis \n",
    "# Voir si dimensions pas trop grandes ?\n",
    "# GRU ! :D mieux (3 params au lieu de 4)\n",
    "# simpleRNN ? \n",
    "# Etude comparative : 3 RNN (simple, LSTM, GRU & Etude de perf)\n",
    "# Limiter Dataset ! => entrainements en O(heure)\n",
    "\n",
    "#tf.keras.utils.get_file(origin=\"lien\", fname=\"nom_que_tu_veux_donner_au_fichier.zip\", extract=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit596a33002c474e61b2acae4601123919"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}