{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from keras import layers \n",
    "from tensorflow.keras.models import Sequential, Model, Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Input, add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the Flickr8k images and caption files\n",
    "dataset_image_path =\"flickr8k/Images/\"\n",
    "dataset_text_path  =\"flickr8k/captions.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wanted shape for images\n",
    "wanted_shape = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the text dataset corresponding to images\n",
    "df_texts = pd.read_csv(dataset_text_path, sep=\",\") #[\"image\",\"caption\"] \n",
    "n_img = df_texts.count()/5 # 40455/5 \n",
    "unique_img = pd.unique(df_texts[\"image\"])# 8091 unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing images with pretrained VGG16 : FEATURE MAPS 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape=wanted_shape, pooling=None, classes=1000,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "# Feature extraction\n",
    "vgg_model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output) #end the modÃ¨le with a 4096 feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_image, one_by_one = False, False # false to gain time when testing other parts\n",
    "# To obtain the feature maps\n",
    "if charge_image:\n",
    "    feature_maps = np.array([model.predict(load_img_from_ds(unique_img[i])) for i in range(len(unique_img))])\n",
    "    print(f\"Shape des fm {feature_maps.shape}\")\n",
    "elif one_by_one:\n",
    "    feature_maps=[]\n",
    "    #for i in range(len(unique_img)):\n",
    "    #    img = load_img_from_ds(unique_img[i])\n",
    "    #    feature_map = model.predict(img)\n",
    "    #    print(feature_map.shape)\n",
    "    #    feature_maps.append(feature_map)\n",
    "    #feature_maps=np.array(feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing captions - WORD2VEC : EMBEDDINGS 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts[\"cleaned\"]=[process_sentence(s) for s in df_texts[\"caption\"]]\n",
    "df_texts[\"cleaned_tokenized\"]=[word_tokenize(w) for w in df_texts[\"cleaned\"]]\n",
    "#df_texts[\"tokenized\"]=character_to_integer_vector(df_texts[\"cleaned\"])\n",
    "word2vec_model = gensim.models.Word2Vec([word_tokenize(w) for w in df_texts[\"cleaned\"]], min_count=1, size=4096)\n",
    "df_texts[\"embedded\"] = word2vec(df_texts,word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACP pour faire correspondre les dimensions du texte et image > on laisse tomber for now\n",
    "\n",
    "#dimages = np.array([np.array(load_img_from_ds(unique_img[i])) for i in range(len(unique_img))])\n",
    "#dimages = np.array([np.array(load_img_from_ds(df_texts[\"image\"][i])) for i in range(len(df_texts[\"image\"]))])\n",
    "dimages = feature_maps\n",
    "\n",
    "# Split du dataset\n",
    "prop_test, prop_val = 0.2, 0.2\n",
    "N = len(df_texts[\"embedded\"])\n",
    "Ntest, Nval = int(N*prop_test), int(N*prop_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = true image caption cleaned\n",
    "dt_test, dt_val, dt_train = split_test_val_train(df_text[\"embedded\"], Ntest, Nval)\n",
    "# di = true image array\n",
    "di_test, di_val, di_train = split_test_val_train(dimages, Ntest, Nval)\n",
    "# fnm = image_name\n",
    "fnm_test, fnm_val, fnm_train = split_test_val_train(df_text[\"image\"], Ntest, Nval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(model.wv.vocab)\n",
    "\n",
    "Xtext_train, Ximage_train, ytext_train = finalpreprocessing(dt_train, di_train, vocab_size) \n",
    "Xtext_val, Ximage_val, ytext_val = finalpreprocessing(dt_val, di_val, vocab_size)\n",
    "\n",
    "print(f\"Training set : \\n \\tInput image : {Ximage_train.shape}\\n\\tInput text : {Xtext_train.shape}\\n\\tOutput text : {ytext_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedding=64\n",
    "\n",
    "# image input\n",
    "input_img = Input(shape=(Ximage_train.shape[1],), name=\"InputImage\") )\n",
    "input_img = ( Dense(units=256,activation='relu',name=\"CompressedImageFeatures\") )(input_img)\n",
    "# text input\n",
    "input_txt = Input(shape=(maxlen,), name=\"InputSequence\"))\n",
    "input_txt = ( Embedding(vocab_size,dim_embedding, mask_zero=True))(input_txt)\n",
    "input_txt = ( LSTM(units=8, activation=\"relu\", name=\"CaptionFeatures\") )(input_txt)\n",
    "\n",
    "# Common part\n",
    "common = add(input_txt, input_img)\n",
    "common = Dense(256, activation='relu') (common)\n",
    "common = Dense(vocab_size, activation='softmax')(common)\n",
    "\n",
    "#Model\n",
    "total_model  = Model(inputs=[input_image, input_txt],outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit([Ximage_train, Xtext_train], ytext_train, epochs=5, verbose=2, batch_size=64, validation_data=([Ximage_val, Xtext_val], ytext_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\", \"val_loss\":\n",
    "    plt.plot(hist.history[label], label=label)]\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PREDICTION\n",
    "'''\n",
    "\n",
    "# 1 couche 256 LSTM ?\n",
    "# A partir de combien de couce=hes c est ok 1 8 16 32 256 \n",
    "# Temps d entrainement : compromis \n",
    "# Voir si dimensions pas trop grandes ?\n",
    "# GRU ! :D mieux (3 params au lieu de 4)\n",
    "# simpleRNN ? \n",
    "# Etude comparative : 3 RNN (simple, LSTM, GRU & Etude de perf)\n",
    "# Limiter Dataset ! => entrainements en O(heure)\n",
    "\n",
    "#tf.keras.utils.get_file(origin=\"lien\", fname=\"nom_que_tu_veux_donner_au_fichier.zip\", extract=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
