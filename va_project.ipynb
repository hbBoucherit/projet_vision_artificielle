{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from tensorflow.keras.models import Sequential, Model, Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Input, add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the Flickr8k images and caption files\n",
    "dataset_image_path =\"flickr8k/Images/\"\n",
    "dataset_text_path  =\"flickr8k/captions.txt\" \n",
    "# Wanted shape for images\n",
    "wanted_shape = (224,224,3)\n",
    "\n",
    "# TO WORK ON A REDUCED DATASET\n",
    "#n_images_considered = 899 # = 8091 / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the text dataset corresponding to images\n",
    "train, infer = True, False\n",
    "\n",
    "if train:\n",
    "    df_texts = pd.read_csv(dataset_text_path, sep=\",\") #[\"image\",\"caption\"] \n",
    "elif infer:\n",
    "    df_texts = pd.read_csv(\"text_feature_maps.csv\") # [\"image\",\"caption\",\"cleaned\",\"cleaned_tokenized\",\"embedded\"]\n",
    "\n",
    "n_img = df_texts.count()/5 # 40455/5 \n",
    "unique_img = pd.unique(df_texts[\"image\"])# 8091 unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing images with pretrained VGG16 : FEATURE MAPS 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape=wanted_shape, pooling=None, classes=1000\n",
    ")\n",
    "# Feature extraction\n",
    "vgg_model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output) #end the mod√®le with a 4096 feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image feature maps loaded : (8091, 4096)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "charge_image, one_by_one, load_fm_image_csv = False, False, True# false to gain time when testing other parts\n",
    "# To obtain the feature maps\n",
    "if charge_image:\n",
    "    feature_maps = np.array([vgg_model.predict(load_img_from_ds(unique_img[i])) for i in range(len(unique_img))])\n",
    "    print(f\"Shape des fm {feature_maps.shape}\")\n",
    "\n",
    "elif one_by_one:\n",
    "    feature_maps=[]\n",
    "    for i in range(len(unique_img)):\n",
    "        if i!=0:\n",
    "            print(f\"{i}/{len(unique_img)} - time elapsed :{time.time()-a}\")\n",
    "        else:\n",
    "            print(f\"{i}/{len(unique_img)}\")\n",
    "        a=time.time()\n",
    "        img = load_img_from_ds(unique_img[i])\n",
    "        feature_map = vgg_model.predict(img)\n",
    "        feature_maps.append(feature_map)\n",
    "    feature_maps=np.array(feature_maps)\n",
    "    #save to csv\n",
    "    feature_maps_sav=feature_maps[:,0,:]\n",
    "    df_fm = pd.DataFrame(feature_maps_sav)\n",
    "    df_fm.to_csv(\"image_feature_maps.csv\")\n",
    "\n",
    "elif load_fm_image_csv:\n",
    "    df_fm = pd.read_csv(\"image_feature_maps.csv\")\n",
    "    feature_maps = np.array(df_fm.drop([df_fm.columns[0]], axis=1))\n",
    "    print(f\"Image feature maps loaded : {feature_maps.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing captions - WORD2VEC : EMBEDDINGS 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 21:00:40,427 : INFO : loading Word2Vec object from word2vec.model\n",
      "2021-01-15 21:00:40,443 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2021-01-15 21:00:40,444 : INFO : loading vectors from word2vec.model.wv.vectors.npy with mmap=None\n",
      "2021-01-15 21:00:40,603 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-01-15 21:00:40,605 : INFO : loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "2021-01-15 21:00:40,605 : INFO : loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "2021-01-15 21:00:40,606 : INFO : loading syn1neg from word2vec.model.trainables.syn1neg.npy with mmap=None\n",
      "2021-01-15 21:00:40,764 : INFO : setting ignored attribute cum_table to None\n",
      "2021-01-15 21:00:40,766 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# Not enough memory yet\n",
    "word2vec_training = False\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    \n",
    "if train :\n",
    "    # Text preprocessing\n",
    "    df_texts[\"cleaned\"]=[process_sentence(s) for s in df_texts[\"caption\"]]\n",
    "    df_texts[\"cleaned_tokenized\"]=[word_tokenize(w) for w in df_texts[\"cleaned\"]]\n",
    "    if(word2vec_training):\n",
    "        word2vec_model = gensim.models.Word2Vec([word_tokenize(w) for w in df_texts[\"cleaned\"]], min_count=1, size=4096)\n",
    "        word2vec_model.save(\"word2vec.model\")\n",
    "    else:\n",
    "        word2vec_model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
    "    text_features = word2vec(df_texts,word2vec_model.wv)\n",
    "    np.save(\"text_feature_maps.npy\",text_features)\n",
    "    #df_texts.to_csv(\"text_feature_maps.csv\", columns=df_texts.columns)\n",
    "\n",
    "elif infer:\n",
    "    word2vec_model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
    "    text_features  = np.load(\"text_feature_maps.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts[\"cleaned\"]=[process_sentence(s) for s in df_texts[\"caption\"]]\n",
    "df_texts[\"cleaned_tokenized\"]=[word_tokenize(w) for w in df_texts[\"cleaned\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts.to_csv(\"df_texts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimages = feature_maps\n",
    "\n",
    "# Split du dataset\n",
    "prop_test, prop_val = 0.2, 0.2\n",
    "N = len(df_texts[\"embedded\"])\n",
    "Ntest, Nval = int(N*prop_test), int(N*prop_val)\n",
    "\n",
    "Nimg = len(dimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_feature_maps(dimages):\n",
    "    Ximage = []\n",
    "    for image in dimages:\n",
    "        for i in range (5):\n",
    "            Ximage.append(image)\n",
    "    Ximage=np.array(Ximage)\n",
    "    return Ximage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeaturemaps = multiple_feature_maps(dimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "def split_test_val_train(df, Ntest, Nval):\n",
    "    return(df[:1000],\n",
    "           df[1000:20000],\n",
    "           df[20000:21000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = true image caption cleaned\n",
    "dt_test, dt_val, dt_train = split_test_val_train(df_texts[\"embedded\"], Ntest, Nval)\n",
    "# di = true image array\n",
    "di_test, di_val, di_train = split_test_val_train(dfeaturemaps, Ntest, Nval)\n",
    "# fnm = image_name\n",
    "fnm_test, fnm_val, fnm_train = split_test_val_train(df_texts[\"image\"], Ntest, Nval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4096)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringtoarray(text):\n",
    "    txt = []\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\", \"\")\n",
    "    text = text.split()\n",
    "    for i in text:\n",
    "        txt.append(float(i))\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocessing(dftext, dfimage, vocab_size):\n",
    "    print(\"# captions/images = {}\".format(len(dftext)))\n",
    "    \n",
    "    maxlen = np.max([len(text) for text in dftext])\n",
    "    Xtext, Ximage, ytext = [], [], []\n",
    "    cpt = 0\n",
    "    for text, image in zip(dftext, dfimage):\n",
    "        cpt = cpt+1\n",
    "        text = stringtoarray(text)\n",
    "        for i in range(0, len(text)):\n",
    "            in_text, out_text = text[:i], text[i]\n",
    "            in_text = pad_sequences([in_text], maxlen=maxlen).flatten()\n",
    "            out_text = to_categorical(out_text, num_classes = vocab_size)\n",
    "            Xtext.append(in_text)\n",
    "            Ximage.append(image)\n",
    "            ytext.append(out_text)\n",
    "    print(cpt)\n",
    "    Xtext = np.array(Xtext)\n",
    "    Ximage = np.array(Ximage)\n",
    "    ytext = np.array(ytext)\n",
    "    return(Xtext, Ximage, ytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# captions/images = 1000\n",
      "1000\n",
      "# captions/images = 19000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2vec_model.wv.vocab)\n",
    "\n",
    "Xtext_train, Ximage_train, ytext_train = finalpreprocessing(dt_train, di_train, vocab_size) \n",
    "Xtext_val, Ximage_val, ytext_val = finalpreprocessing(dt_val, di_val, vocab_size)\n",
    "\n",
    "print(f\"Training set : \\n \\tInput image : {Ximage_train.shape}\\n\\tInput text : {Xtext_train.shape}\\n\\tOutput text : {ytext_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-294b8f3276fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# image input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0minput_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXimage_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"InputImage\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0minput_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"CompressedImageFeatures\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# text input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "dim_embedding=64\n",
    "\n",
    "# image input\n",
    "input_img = Input(shape=(Ximage_train.shape[1],), name=\"InputImage\")\n",
    "input_img = ( Dense(units=256,activation='relu',name=\"CompressedImageFeatures\"))(input_img)\n",
    "# text input\n",
    "input_txt = Input(shape=(maxlen,), name=\"InputSequence\")\n",
    "input_txt = ( Embedding(vocab_size,dim_embedding, mask_zero=True))(input_txt)\n",
    "input_txt = ( LSTM(units=8, activation=\"relu\", name=\"CaptionFeatures\"))(input_txt)\n",
    "\n",
    "# Common part\n",
    "common = add(input_txt, input_img)\n",
    "common = Dense(256, activation='relu')(common)\n",
    "common = Dense(vocab_size, activation='softmax')(common)\n",
    "\n",
    "#Model\n",
    "total_model  = Model(inputs=[input_image, input_txt],outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit([Ximage_train, Xtext_train], ytext_train, epochs=5, verbose=2, batch_size=64, validation_data=([Ximage_val, Xtext_val], ytext_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\", \"val_loss\"]:\n",
    "    plt.plot(hist.history[label], label=label)]\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PREDICTION\n",
    "'''\n",
    "\n",
    "# 1 couche 256 LSTM ?\n",
    "# A partir de combien de couce=hes c est ok 1 8 16 32 256 \n",
    "# Temps d entrainement : compromis \n",
    "# Voir si dimensions pas trop grandes ?\n",
    "# GRU ! :D mieux (3 params au lieu de 4)\n",
    "# simpleRNN ? \n",
    "# Etude comparative : 3 RNN (simple, LSTM, GRU & Etude de perf)\n",
    "# Limiter Dataset ! => entrainements en O(heure)\n",
    "\n",
    "#tf.keras.utils.get_file(origin=\"lien\", fname=\"nom_que_tu_veux_donner_au_fichier.zip\", extract=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
